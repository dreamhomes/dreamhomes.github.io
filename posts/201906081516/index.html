<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>PyTorch 常用函数解析 | 梦家博客</title><meta name="keywords" content="PyTorch"><meta name="author" content="梦家"><meta name="copyright" content="梦家"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="detach() 和 .data PyTorch0.4中，.data 仍保留，但建议使用 .detach(), 区别在于 .data 返回和 x 的相同数据 tensor, 但不会加入到x的计算历史里，且require s_grad &#x3D; False, 这样有些时候是不安全的, 因为 x.data 不能被 autograd 追踪求微分 。 .detach() 返回相同数据的 tensor ,且 re"><meta property="og:type" content="article"><meta property="og:title" content="PyTorch 常用函数解析"><meta property="og:url" content="https://dreamhomes.top/posts/201906081516/"><meta property="og:site_name" content="梦家博客"><meta property="og:description" content="detach() 和 .data PyTorch0.4中，.data 仍保留，但建议使用 .detach(), 区别在于 .data 返回和 x 的相同数据 tensor, 但不会加入到x的计算历史里，且require s_grad &#x3D; False, 这样有些时候是不安全的, 因为 x.data 不能被 autograd 追踪求微分 。 .detach() 返回相同数据的 tensor ,且 re"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg"><meta property="article:published_time" content="2019-06-08T15:16:40.000Z"><meta property="article:modified_time" content="2022-05-26T03:15:49.144Z"><meta property="article:author" content="梦家"><meta property="article:tag" content="PyTorch"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/favicon.png"><link rel="canonical" href="https://dreamhomes.top/posts/201906081516/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:20,languages:{author:"作者: 梦家",link:"链接: ",source:"来源: 梦家博客",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#6F42C1",bgDark:"#6F42C1",position:"bottom-left"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!0,islazyload:!0,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-05-26 03:15:49"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const a=864e5*o,c={value:t,expiry:(new Date).getTime()+a};localStorage.setItem(e,JSON.stringify(c))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const a=document.createElement("script");a.src=e,a.async=!0,a.onerror=o,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=window.matchMedia("(prefers-color-scheme: dark)").matches,a=window.matchMedia("(prefers-color-scheme: light)").matches,c=window.matchMedia("(prefers-color-scheme: no-preference)").matches,n=!o&&!a&&!c;if(void 0===t){if(a)activateLightMode();else if(o)activateDarkMode();else if(c||n){const e=(new Date).getHours();e<=6||e>=18?activateDarkMode():activateLightMode()}window.matchMedia("(prefers-color-scheme: dark)").addListener((function(e){void 0===saveToLocal.get("theme")&&(e.matches?activateDarkMode():activateLightMode())}))}else"light"===t?activateLightMode():activateDarkMode();const i=saveToLocal.get("aside-status");void 0!==i&&("hide"===i?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));const r=saveToLocal.get("global-font-size");void 0!==r&&document.documentElement.style.setProperty("--global-font-size",r+"px")})(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/css/custom/twikoo_beautify.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 5.4.1"><link rel="alternate" href="/atom.xml" title="梦家博客" type="application/atom+xml"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/avatar.gif" onerror='onerror=null,src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">183</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">63</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i> <span>娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i> <span>书单</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comments"></i> <span>留言</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://sites.google.com/site/mengjiashen01/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">梦家博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i> <span>娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i> <span>书单</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comments"></i> <span>留言</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://sites.google.com/site/mengjiashen01/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch 常用函数解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-06-08T15:16:40.000Z" title="发表于 2019-06-08 15:16:40">2019-06-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-05-26T03:15:49.144Z" title="更新于 2022-05-26 03:15:49">2022-05-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/">技术杂谈</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/201906081516/#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="detach-和-data">detach() 和 .data</h2><p>PyTorch0.4中，<code>.data</code> 仍保留，但建议使用 <code>.detach()</code>, 区别在于 <code>.data</code> 返回和 <code>x</code> 的相同数据 <code>tensor</code>, 但不会加入到<code>x</code>的计算历史里，且<code>require s_grad = False</code>, 这样有些时候是不安全的, 因为 <code>x.data</code> 不能被 <code>autograd</code> 追踪求微分 。 <code>.detach()</code> 返回相同数据的 <code>tensor</code> ,且 <code>requires_grad=False</code> ,但能通过 <code>in-place</code> 操作报告给 <code>autograd</code> 在进行反向传播的时候.<br>举例：<br><strong>tensor.data</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor([1,2,3.], requires_grad =True)</span><br><span class="line">&gt;&gt;&gt; out = a.sigmoid()</span><br><span class="line">&gt;&gt;&gt; c = out.data</span><br><span class="line">&gt;&gt;&gt; c.zero_()</span><br><span class="line">tensor([ 0., 0., 0.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; out                   <span class="comment">#  out的数值被c.zero_()修改</span></span><br><span class="line">tensor([ 0., 0., 0.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; out.sum().backward()  <span class="comment">#  反向传播</span></span><br><span class="line">&gt;&gt;&gt; a.grad                <span class="comment">#  这个结果很严重的错误，因为out已经改变了</span></span><br><span class="line">tensor([ 0., 0., 0.])</span><br></pre></td></tr></table></figure><p><strong>tensor.detach()</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.tensor([1,2,3.], requires_grad =True)</span><br><span class="line">&gt;&gt;&gt; out = a.sigmoid()</span><br><span class="line">&gt;&gt;&gt; c = out.detach()</span><br><span class="line">&gt;&gt;&gt; c.zero_()</span><br><span class="line">tensor([ 0., 0., 0.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; out                   <span class="comment">#  out的值被c.zero_()修改 !!</span></span><br><span class="line">tensor([ 0., 0., 0.])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; out.sum().backward()  <span class="comment">#  需要原来out得值，但是已经被c.zero_()覆盖了，结果报错</span></span><br><span class="line">RuntimeError: one of the variables needed <span class="keyword">for</span> gradient</span><br><span class="line">computation has been modified by an</span><br></pre></td></tr></table></figure><h2 id="torch-cat-张量拼接">torch.cat() 张量拼接</h2><p>对张量沿着某一维度进行拼接。连接后数据的总维数不变。，ps:能拼接的前提是<strong>对应的维度相同！！！</strong></p><p>例如对两个2维tensor（分别为<code>2*3</code>,<code>1*3</code>）进行拼接，拼接完后变为<code>3*3</code>的2维 tensor。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: &lt;torch._C.Generator at <span class="number">0x19e56f02e50</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: x = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: y = torch.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: x</span><br><span class="line">Out[<span class="number">5</span>]:</span><br><span class="line">tensor([[ <span class="number">0.6614</span>,  <span class="number">0.2669</span>,  <span class="number">0.0617</span>],</span><br><span class="line">        [ <span class="number">0.6213</span>, -<span class="number">0.4519</span>, -<span class="number">0.1661</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: y</span><br><span class="line">Out[<span class="number">6</span>]: tensor([[-<span class="number">1.5228</span>,  <span class="number">0.3817</span>, -<span class="number">1.0276</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: torch.cat((x,y),<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">9</span>]:</span><br><span class="line">tensor([[ <span class="number">0.6614</span>,  <span class="number">0.2669</span>,  <span class="number">0.0617</span>],</span><br><span class="line">        [ <span class="number">0.6213</span>, -<span class="number">0.4519</span>, -<span class="number">0.1661</span>],</span><br><span class="line">        [-<span class="number">1.5228</span>,  <span class="number">0.3817</span>, -<span class="number">1.0276</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以上<code>dim=0</code> 表示按列进行拼接，<code>dim=1</code>表示按行进行拼接。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">11</span>]: z = torch.randn(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: z</span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">tensor([[-<span class="number">0.5631</span>, -<span class="number">0.8923</span>],</span><br><span class="line">        [-<span class="number">0.0583</span>, -<span class="number">0.1955</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: x</span><br><span class="line">Out[<span class="number">13</span>]:</span><br><span class="line">tensor([[ <span class="number">0.6614</span>,  <span class="number">0.2669</span>,  <span class="number">0.0617</span>],</span><br><span class="line">        [ <span class="number">0.6213</span>, -<span class="number">0.4519</span>, -<span class="number">0.1661</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: torch.cat((x,z),<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">14</span>]:</span><br><span class="line">tensor([[ <span class="number">0.6614</span>,  <span class="number">0.2669</span>,  <span class="number">0.0617</span>, -<span class="number">0.5631</span>, -<span class="number">0.8923</span>],</span><br><span class="line">        [ <span class="number">0.6213</span>, -<span class="number">0.4519</span>, -<span class="number">0.1661</span>, -<span class="number">0.0583</span>, -<span class="number">0.1955</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="torch-stack-张量堆叠">torch.stack() 张量堆叠</h2><p><code>torch.cat()</code>拼接不会增加新的维度，但<code>torch.stack()</code>则会增加新的维度。</p><p>例如对两个<code>1*2</code> 维的 tensor 在第0个维度上stack，则会变为<code>2*1*2</code>的 tensor；在第1个维度上stack，则会变为<code>1*2*2</code> 的tensor。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">22</span>]: x = torch.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: y = torch.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: x.shape</span><br><span class="line">Out[<span class="number">24</span>]: torch.Size([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: x = torch.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: y = torch.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: torch.stack((x,y),<span class="number">0</span>) <span class="comment"># 维度0堆叠</span></span><br><span class="line">Out[<span class="number">27</span>]:</span><br><span class="line">tensor([[[-<span class="number">1.8313</span>,  <span class="number">1.5987</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">1.2770</span>,  <span class="number">0.3255</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: torch.stack((x,y),<span class="number">0</span>).shape</span><br><span class="line">Out[<span class="number">28</span>]: torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: torch.stack((x,y),<span class="number">1</span>) <span class="comment"># 维度1堆叠</span></span><br><span class="line">Out[<span class="number">29</span>]:</span><br><span class="line">tensor([[[-<span class="number">1.8313</span>,  <span class="number">1.5987</span>],</span><br><span class="line">         [-<span class="number">1.2770</span>,  <span class="number">0.3255</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: torch.stack((x,y),<span class="number">1</span>).shape</span><br><span class="line">Out[<span class="number">30</span>]: torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="torch-transpose-矩阵转置">torch.transpose() 矩阵转置</h2><p>举例说明</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">x = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>原来x的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">0.6614</span>  <span class="number">0.2669</span>  <span class="number">0.0617</span></span><br><span class="line"> <span class="number">0.6213</span> -<span class="number">0.4519</span> -<span class="number">0.1661</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure><p>将x的维度互换:<code>x.transpose(0,1)</code> ，其实相当于转置操作！<br>结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.6614</span>  <span class="number">0.6213</span></span><br><span class="line"> <span class="number">0.2669</span> -<span class="number">0.4519</span></span><br><span class="line"> <span class="number">0.0617</span> -<span class="number">0.1661</span></span><br><span class="line">[torch.FloatTensor of size 3x2]</span><br></pre></td></tr></table></figure><h2 id="torch-permute-多维度互换">torch.permute() 多维度互换</h2><p>permute是更灵活的transpose，可以灵活的对原数据的维度进行调换，而数据本身不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">31</span>]: x = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: x</span><br><span class="line">Out[<span class="number">32</span>]:</span><br><span class="line">tensor([[[ <span class="number">0.7626</span>,  <span class="number">0.4415</span>,  <span class="number">1.1651</span>,  <span class="number">2.0154</span>],</span><br><span class="line">         [ <span class="number">0.2152</span>, -<span class="number">0.5242</span>, -<span class="number">1.8034</span>, -<span class="number">1.3083</span>],</span><br><span class="line">         [ <span class="number">0.4100</span>,  <span class="number">0.4085</span>,  <span class="number">0.2579</span>,  <span class="number">1.0950</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.5065</span>,  <span class="number">0.0998</span>, -<span class="number">0.6540</span>,  <span class="number">0.7317</span>],</span><br><span class="line">         [-<span class="number">1.4567</span>,  <span class="number">1.6089</span>,  <span class="number">0.0938</span>, -<span class="number">1.2597</span>],</span><br><span class="line">         [ <span class="number">0.2546</span>, -<span class="number">0.5020</span>, -<span class="number">1.0412</span>,  <span class="number">0.7323</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: x.shape</span><br><span class="line">Out[<span class="number">33</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: x.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>) <span class="comment"># 0维和1维互换，2维不变！</span></span><br><span class="line">Out[<span class="number">34</span>]:</span><br><span class="line">tensor([[[ <span class="number">0.7626</span>,  <span class="number">0.4415</span>,  <span class="number">1.1651</span>,  <span class="number">2.0154</span>],</span><br><span class="line">         [-<span class="number">0.5065</span>,  <span class="number">0.0998</span>, -<span class="number">0.6540</span>,  <span class="number">0.7317</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.2152</span>, -<span class="number">0.5242</span>, -<span class="number">1.8034</span>, -<span class="number">1.3083</span>],</span><br><span class="line">         [-<span class="number">1.4567</span>,  <span class="number">1.6089</span>,  <span class="number">0.0938</span>, -<span class="number">1.2597</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.4100</span>,  <span class="number">0.4085</span>,  <span class="number">0.2579</span>,  <span class="number">1.0950</span>],</span><br><span class="line">         [ <span class="number">0.2546</span>, -<span class="number">0.5020</span>, -<span class="number">1.0412</span>,  <span class="number">0.7323</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">35</span>]: x.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>).shape</span><br><span class="line">Out[<span class="number">35</span>]: torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="torch-squeeze-和-torch-unsqueeze">torch.squeeze() 和 torch.unsqueeze()</h2><p>常用来增加或减少维度,如没有batch维度时，增加batch维度为1。</p><ul><li>squeeze(dim_n)压缩，减少dim_n维度 ，即去掉元素数量为1的dim_n维度。</li><li>unsqueeze(dim_n)，增加dim_n维度，元素数量为1。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">38</span>]: x = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: x.shape</span><br><span class="line">Out[<span class="number">39</span>]: torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">40</span>]: x</span><br><span class="line">Out[<span class="number">40</span>]:</span><br><span class="line">tensor([[[-<span class="number">0.4791</span>,  <span class="number">0.2912</span>, -<span class="number">0.8317</span>, -<span class="number">0.5525</span>],</span><br><span class="line">         [ <span class="number">0.6355</span>, -<span class="number">0.3968</span>, -<span class="number">0.6571</span>, -<span class="number">1.6428</span>],</span><br><span class="line">         [ <span class="number">0.9803</span>, -<span class="number">0.0421</span>, -<span class="number">0.8206</span>,  <span class="number">0.3133</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">41</span>]: x.squeeze()</span><br><span class="line">Out[<span class="number">41</span>]:</span><br><span class="line">tensor([[-<span class="number">0.4791</span>,  <span class="number">0.2912</span>, -<span class="number">0.8317</span>, -<span class="number">0.5525</span>],</span><br><span class="line">        [ <span class="number">0.6355</span>, -<span class="number">0.3968</span>, -<span class="number">0.6571</span>, -<span class="number">1.6428</span>],</span><br><span class="line">        [ <span class="number">0.9803</span>, -<span class="number">0.0421</span>, -<span class="number">0.8206</span>,  <span class="number">0.3133</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">42</span>]: x.squeeze().shape</span><br><span class="line">Out[<span class="number">42</span>]: torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">43</span>]: x.unsqueeze(<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">43</span>]:</span><br><span class="line">tensor([[[[-<span class="number">0.4791</span>,  <span class="number">0.2912</span>, -<span class="number">0.8317</span>, -<span class="number">0.5525</span>],</span><br><span class="line">          [ <span class="number">0.6355</span>, -<span class="number">0.3968</span>, -<span class="number">0.6571</span>, -<span class="number">1.6428</span>],</span><br><span class="line">          [ <span class="number">0.9803</span>, -<span class="number">0.0421</span>, -<span class="number">0.8206</span>,  <span class="number">0.3133</span>]]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">44</span>]: x.unsqueeze(<span class="number">0</span>).shape</span><br><span class="line">Out[<span class="number">44</span>]: torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><blockquote><p><code>torch.Tensor</code>有两个实例方法可以用来扩展某维的数据的尺寸，分别是 <code>repeat()</code>和 <code>expand()</code>。</p></blockquote><h2 id="expand">expand()</h2><p>返回当前张量在某维扩展更大后的张量。按照指定size扩充。</p><p>扩展（expand）张量不会分配新的内存，只是在存在的张量上创建一个新的视图（view），一个大小（size）等于1的维度扩展到更大的尺寸。<br>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">45</span>]: x = torch.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">46</span>]: x</span><br><span class="line">Out[<span class="number">46</span>]: tensor([[-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: x.expand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">47</span>]:</span><br><span class="line">tensor([[-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>],</span><br><span class="line">        [-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: x.expand(<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">48</span>]:</span><br><span class="line">tensor([[-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>],</span><br><span class="line">        [-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="repeat">repeat()</h2><p>沿着特定的维度重复这个张量，按照倍数扩充；和<code>expand()</code>不同的是，这个函数拷贝张量的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">53</span>]: x</span><br><span class="line">Out[<span class="number">53</span>]: tensor([[-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">54</span>]: x.shape</span><br><span class="line">Out[<span class="number">54</span>]: torch.Size([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">55</span>]: x.repeat(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">55</span>]:</span><br><span class="line">tensor([[-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>, -<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>, -<span class="number">1.1352</span>,  <span class="number">0.3773</span>,</span><br><span class="line">         -<span class="number">0.2824</span>],</span><br><span class="line">        [-<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>, -<span class="number">1.1352</span>,  <span class="number">0.3773</span>, -<span class="number">0.2824</span>, -<span class="number">1.1352</span>,  <span class="number">0.3773</span>,</span><br><span class="line">         -<span class="number">0.2824</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">56</span>]: x.repeat(<span class="number">2</span>,<span class="number">3</span>).shape</span><br><span class="line">Out[<span class="number">56</span>]: torch.Size([<span class="number">2</span>, <span class="number">9</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="torch-narrow">torch.narrow()</h2><p>PyTorch 中的<code>narrow()</code>函数起到了筛选一定维度上的数据作用。个人感觉与<code>x[begin:end]</code> 相同！</p><p>参考官网：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/master/generated/torch.narrow.html">torch.narrow()</a></p><p>用法：<code>torch.narrow(input, dim, start, length) → Tensor</code></p><p>返回输入张量的切片操作结果。 输入tensor和返回的tensor共享内存。</p><p>参数说明：</p><ul><li><code>input (Tensor)</code> – 需切片的张量</li><li><code>dim (int)</code> – 切片维度</li><li><code>start (int)</code> – 开始的索引</li><li><code>length (int)</code> – 切片长度</li></ul><p>示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: x = torch.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: x</span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">tensor([[ <span class="number">1.2474</span>,  <span class="number">0.1820</span>, -<span class="number">0.0179</span>],</span><br><span class="line">        [ <span class="number">0.1388</span>, -<span class="number">1.7373</span>,  <span class="number">0.5934</span>],</span><br><span class="line">        [ <span class="number">0.2288</span>,  <span class="number">1.1102</span>,  <span class="number">0.6743</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: x.narrow(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># 行切片</span></span><br><span class="line">Out[<span class="number">4</span>]:</span><br><span class="line">tensor([[ <span class="number">0.1388</span>, -<span class="number">1.7373</span>,  <span class="number">0.5934</span>],</span><br><span class="line">        [ <span class="number">0.2288</span>,  <span class="number">1.1102</span>,  <span class="number">0.6743</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: x.narrow(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># 列切片</span></span><br><span class="line">Out[<span class="number">5</span>]:</span><br><span class="line">tensor([[ <span class="number">0.1820</span>, -<span class="number">0.0179</span>],</span><br><span class="line">        [-<span class="number">1.7373</span>,  <span class="number">0.5934</span>],</span><br><span class="line">        [ <span class="number">1.1102</span>,  <span class="number">0.6743</span>]])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="torch-unbind">torch.unbind()</h2><p><code>torch.unbind()</code>移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片。</p><p>参考官网：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/master/generated/torch.unbind.html">torch.unbind()</a></p><p>用法：<code>torch.unbind(input, dim=0) → seq</code></p><p>返回指定维度切片后的元组。</p><p>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: x</span><br><span class="line">Out[<span class="number">6</span>]:</span><br><span class="line">tensor([[ <span class="number">1.2474</span>,  <span class="number">0.1820</span>, -<span class="number">0.0179</span>],</span><br><span class="line">        [ <span class="number">0.1388</span>, -<span class="number">1.7373</span>,  <span class="number">0.5934</span>],</span><br><span class="line">        [ <span class="number">0.2288</span>,  <span class="number">1.1102</span>,  <span class="number">0.6743</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: torch.unbind(x, <span class="number">0</span>)</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">(tensor([ <span class="number">1.2474</span>,  <span class="number">0.1820</span>, -<span class="number">0.0179</span>]),</span><br><span class="line"> tensor([ <span class="number">0.1388</span>, -<span class="number">1.7373</span>,  <span class="number">0.5934</span>]),</span><br><span class="line"> tensor([<span class="number">0.2288</span>, <span class="number">1.1102</span>, <span class="number">0.6743</span>]))</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: torch.unbind(x, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">(tensor([<span class="number">1.2474</span>, <span class="number">0.1388</span>, <span class="number">0.2288</span>]),</span><br><span class="line"> tensor([ <span class="number">0.1820</span>, -<span class="number">1.7373</span>,  <span class="number">1.1102</span>]),</span><br><span class="line"> tensor([-<span class="number">0.0179</span>,  <span class="number">0.5934</span>,  <span class="number">0.6743</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="torch-gather">torch.gather()</h2><p>参考官网：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#torch.gather">torch.gather</a></p><p>用法：<code>torch.gather(input, dim, index, out=None, sparse_grad=False) → Tensor</code></p><p>收集输入的特定维度<code>dim</code>指定位置<code>index</code>的数值。</p><p>对于一个三维tensor，结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0</span><br><span class="line">out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1</span><br><span class="line">out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><code>input (Tensor)</code> – 输入张量</li><li><code>dim (int)</code> – 索引维度</li><li><code>index (LongTensor)</code> – 收集元素的索引</li><li><code>out (Tensor, optional)</code> – 目标张量</li><li><code>sparse_grad (bool,optional)</code> – 输入为稀疏张量</li></ul><p>直接看官网解释有点不明白，参考另一文章的实例说明：<a target="_blank" rel="noopener" href="https://blog.csdn.net/cpluss/article/details/90260550">https://blog.csdn.net/cpluss/article/details/90260550</a></p><p>在序列标注问题上，我们给每一个单词都标上一个标签。不妨假设我们有4个句子，每个句子的长度不一定相同，标签如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input = [</span><br><span class="line">    [2, 3, 4, 5],</span><br><span class="line">    [1, 4, 3],</span><br><span class="line">    [4, 2, 2, 5, 7],</span><br><span class="line">    [1]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>上例中有四个句子，长度分别为4，3，5，1，其中第一个句子的标签为2，3，4，5。我们知道，处理自然语言问题时，一般都需要进行padding，即将不同长度的句子padding到同一长度，以0为padding，那么上述经padding后变为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input = [</span><br><span class="line">    [2, 3, 4, 5, 0, 0],</span><br><span class="line">    [1, 4, 3, 0, 0, 0],</span><br><span class="line">    [4, 2, 2, 5, 7, 0],</span><br><span class="line">    [1, 0, 0, 0, 0, 0]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>那么问题来了，现在我们想获得每个句子中最后一个词语的标签，该怎么得到呢？既是，第一句话中的5，第二句话中的3，第三句话中7，第四句话中的1。</p><p>此时就需要用gather函数。</p><p>此时我们的input就是填充之后的tensor，dim=1, index就是各个句子的长度，即[[4],[3],[5],[1]]。之所以维度是4*1，是为了满足index维度和input维度之间的关系（讲解参数时有讲）。</p><p>代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">26</span>]: <span class="keyword">import</span> torch</span><br><span class="line">    ...: <span class="built_in">input</span> = [</span><br><span class="line">    ...:     [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    ...:     [<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    ...:     [<span class="number">4</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">0</span>],</span><br><span class="line">    ...:     [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    ...: ]</span><br><span class="line">    ...: <span class="built_in">input</span> = torch.tensor(<span class="built_in">input</span>)</span><br><span class="line">    ...: <span class="comment">#注意index的类型</span></span><br><span class="line">    ...: length = torch.LongTensor([[<span class="number">4</span>],[<span class="number">3</span>],[<span class="number">5</span>],[<span class="number">1</span>]])</span><br><span class="line">    ...: <span class="comment">#index之所以减1,是因为序列维度是从0开始计算的</span></span><br><span class="line">    ...: out = torch.gather(<span class="built_in">input</span>, <span class="number">1</span>, length-<span class="number">1</span>)</span><br><span class="line">    ...: out</span><br><span class="line">Out[<span class="number">26</span>]:</span><br><span class="line">tensor([[<span class="number">5</span>],</span><br><span class="line">        [<span class="number">3</span>],</span><br><span class="line">        [<span class="number">7</span>],</span><br><span class="line">        [<span class="number">1</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Contact">Contact</h2><img src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/self-simple.png" style="zoom:67%"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">梦家</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://dreamhomes.top/posts/201906081516/">https://dreamhomes.top/posts/201906081516/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dreamhomes.top" target="_blank">梦家博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/wechat.png" target="_blank"><img class="post-qr-code-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/wechat.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/alipay.jpg" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/201906121707/"><img class="prev-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img10.jpeg" onerror='onerror=null,src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">有向图中的随机游走方法</div></div></a></div><div class="next-post pull-right"><a href="/posts/201906032136/"><img class="next-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img1.jpeg" onerror='onerror=null,src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习中的 KL 散度及其 Python 实现</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/201812302155/" title="PyTorch 高效编程手册"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img7.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-30</div><div class="title">PyTorch 高效编程手册</div></div></a></div><div><a href="/posts/201902220946/" title="PyTorch 简明学习教程与高效编程技巧"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img13.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-02-22</div><div class="title">PyTorch 简明学习教程与高效编程技巧</div></div></a></div><div><a href="/posts/202011261041/" title="PyTorch 中的损失函数总结"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img5.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-26</div><div class="title">PyTorch 中的损失函数总结</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/avatar.gif" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/friend_404.gif"' alt="avatar"><div class="author-info__name">梦家</div><div class="author-info__description">算法工程师｜机器学习<br><br>前 途 似 海 渺 渺<br>来 日 方 长 漫 漫<br>与 其 互 为 人 间<br>不 如 自 成 宇 宙</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">183</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">63</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dreamhomes"><i class="fab fa-github"></i><span>Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/dreamhomes" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:shenmj13@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/DreamHome_S" target="_blank" title="CSDN"><i class="fab fa-cuttlefish"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>重要通知</span></div><div class="announcement_content">如对本站内容存在任何疑问 🧐 欢迎 <a href="https://dreamhomes.top/comments/">留言</a> 反馈或者联系作者<br><br><center><img src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/20210910100633.png" alt="联系博主" style="zoom:72%"></center></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#detach-%E5%92%8C-data"><span class="toc-number">1.</span> <span class="toc-text">detach() 和 .data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-cat-%E5%BC%A0%E9%87%8F%E6%8B%BC%E6%8E%A5"><span class="toc-number">2.</span> <span class="toc-text">torch.cat() 张量拼接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-stack-%E5%BC%A0%E9%87%8F%E5%A0%86%E5%8F%A0"><span class="toc-number">3.</span> <span class="toc-text">torch.stack() 张量堆叠</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-transpose-%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE"><span class="toc-number">4.</span> <span class="toc-text">torch.transpose() 矩阵转置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-permute-%E5%A4%9A%E7%BB%B4%E5%BA%A6%E4%BA%92%E6%8D%A2"><span class="toc-number">5.</span> <span class="toc-text">torch.permute() 多维度互换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-squeeze-%E5%92%8C-torch-unsqueeze"><span class="toc-number">6.</span> <span class="toc-text">torch.squeeze() 和 torch.unsqueeze()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#expand"><span class="toc-number">7.</span> <span class="toc-text">expand()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#repeat"><span class="toc-number">8.</span> <span class="toc-text">repeat()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-narrow"><span class="toc-number">9.</span> <span class="toc-text">torch.narrow()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-unbind"><span class="toc-number">10.</span> <span class="toc-text">torch.unbind()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-gather"><span class="toc-number">11.</span> <span class="toc-text">torch.gather()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Contact"><span class="toc-number">12.</span> <span class="toc-text">Contact</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/202204281516/" title="智能运维 AIOps 系列丨调用链根因定位算法综述"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img12.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="智能运维 AIOps 系列丨调用链根因定位算法综述"></a><div class="content"><a class="title" href="/posts/202204281516/" title="智能运维 AIOps 系列丨调用链根因定位算法综述">智能运维 AIOps 系列丨调用链根因定位算法综述</a><time datetime="2022-04-28T15:16:58.000Z" title="发表于 2022-04-28 15:16:58">2022-04-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204241101/" title="Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img6.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析"></a><div class="content"><a class="title" href="/posts/202204241101/" title="Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析">Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析</a><time datetime="2022-04-24T11:01:59.000Z" title="发表于 2022-04-24 11:01:59">2022-04-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204221627/" title="ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img9.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法"></a><div class="content"><a class="title" href="/posts/202204221627/" title="ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法">ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法</a><time datetime="2022-04-22T16:27:43.000Z" title="发表于 2022-04-22 16:27:43">2022-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204221003/" title="齐夫定律 (Zipf' law) 理论及其应用场景"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img6.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="齐夫定律 (Zipf' law) 理论及其应用场景"></a><div class="content"><a class="title" href="/posts/202204221003/" title="齐夫定律 (Zipf' law) 理论及其应用场景">齐夫定律 (Zipf' law) 理论及其应用场景</a><time datetime="2022-04-22T10:03:07.000Z" title="发表于 2022-04-22 10:03:07">2022-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204201700/" title="大规模图可视化分析框架 Graphistry"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="大规模图可视化分析框架 Graphistry"></a><div class="content"><a class="title" href="/posts/202204201700/" title="大规模图可视化分析框架 Graphistry">大规模图可视化分析框架 Graphistry</a><time datetime="2022-04-20T17:00:42.000Z" title="发表于 2022-04-20 17:00:42">2022-04-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 梦家</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/"><img src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/icp.png"> <span>赣ICP备 20003744 号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>document.getElementsByClassName("mermaid").length&&(window.mermaidJsLoad?mermaid.init():getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(()=>{window.mermaidJsLoad=!0,mermaid.initialize({theme:"default"})}))</script><script>(()=>{const o=document.getElementById("twikoo-count"),t=()=>{let o={el:"#twikoo-wrap",envId:"hello-cloudbase-7gmvk17h649821e3",region:""};twikoo.init(o)},e=()=>{twikoo.getCommentsCount({envId:"hello-cloudbase-7gmvk17h649821e3",region:"",urls:[window.location.pathname],includeReply:!1}).then((function(t){o.innerText=t[0].count})).catch((function(o){console.error(o)}))},n=(n=!1)=>{"object"==typeof twikoo?(t(),n&&o&&setTimeout(e,0)):getScript("https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js").then(()=>{t(),n&&o&&setTimeout(e,0)})};n(!0)})()</script><script>function loadValine(){function e(){let e={el:"#vcomment",appId:"1JeI4r7MuSNuHIKgevu6uP8q-gzGzoHsz",appKey:"Ykl58TSExNWvTIyBTbnmp26h",placeholder:"💗 Hi, see you tomorrow. 💗",avatar:"monsterid",meta:"nick,mail,link".split(","),pageSize:"10",lang:"zh-CN",recordIP:!1,serverURLs:"https://1jei4r7m.lc-cn-n1-shared.com",emojiCDN:"",emojiMaps:"",enableQQ:!1,path:window.location.pathname,master:"7a7f78ed775ede7059cf68eb46898cc5".split(","),friends:"5d7570d1f1fefc9918c797ee65af3899,ec18c657f7ea8128e4bb73e96e45d236,d304727a8d439e45b08f8579666ee818".split(","),tagMeta:"博主,伙伴,访客".split(",")};e.requiredFields="nick,mail".split(",");new Valine(e)}"function"==typeof Valine?e():getScript("https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js").then(e)}{function loadOtherComment(){loadValine()}setTimeout(loadValine,0)}</script></div><script defer id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>function gitcalendar_injector_config(){document.getElementById("recent-posts").insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>'),console.log("已挂载gitcalendar")}document.getElementById("recent-posts")&&"/"===location.pathname&&(gitcalendar_injector_config(),GitCalendarInit("https://gitcalendar.akilar.top/api?dreamhomes",["#ebedf0","#fdcdec","#fc9bd9","#fa6ac5","#f838b2","#f5089f","#c4067e","#92055e","#540336","#48022f","#30021f"],"dreamhomes"))</script></body></html>