<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>PyTorch 简明学习教程与高效编程技巧 | 梦家博客</title><meta name="keywords" content="PyTorch"><meta name="author" content="梦家"><meta name="copyright" content="梦家"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="背景 邻近毕业季事情多且杂，已很长时间未接触PyTorch编程了，不动手实践以前学习的内容就慢慢淡忘了，而且现在PyTorch已更新到版本1.5.0，回想起最开始学习PyTorch时还是版本0.4.1只能感叹技术迭代和AI发展的速度太快了啊！以前学习的 tutorials 换电脑时也丢了…遂… 本篇文章首先简单过一遍PyTorch基础内容，再结合 Github 上的 tutorials 学习一些高"><meta property="og:type" content="article"><meta property="og:title" content="PyTorch 简明学习教程与高效编程技巧"><meta property="og:url" content="https://dreamhomes.top/posts/201902220946/"><meta property="og:site_name" content="梦家博客"><meta property="og:description" content="背景 邻近毕业季事情多且杂，已很长时间未接触PyTorch编程了，不动手实践以前学习的内容就慢慢淡忘了，而且现在PyTorch已更新到版本1.5.0，回想起最开始学习PyTorch时还是版本0.4.1只能感叹技术迭代和AI发展的速度太快了啊！以前学习的 tutorials 换电脑时也丢了…遂… 本篇文章首先简单过一遍PyTorch基础内容，再结合 Github 上的 tutorials 学习一些高"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img13.jpeg"><meta property="article:published_time" content="2019-02-22T09:46:57.000Z"><meta property="article:modified_time" content="2022-05-26T03:15:49.144Z"><meta property="article:author" content="梦家"><meta property="article:tag" content="PyTorch"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img13.jpeg"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/favicon.png"><link rel="canonical" href="https://dreamhomes.top/posts/201902220946/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:20,languages:{author:"作者: 梦家",link:"链接: ",source:"来源: 梦家博客",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#6F42C1",bgDark:"#6F42C1",position:"bottom-left"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!0,islazyload:!0,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-05-26 03:15:49"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const a=864e5*o,c={value:t,expiry:(new Date).getTime()+a};localStorage.setItem(e,JSON.stringify(c))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const a=document.createElement("script");a.src=e,a.async=!0,a.onerror=o,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=window.matchMedia("(prefers-color-scheme: dark)").matches,a=window.matchMedia("(prefers-color-scheme: light)").matches,c=window.matchMedia("(prefers-color-scheme: no-preference)").matches,n=!o&&!a&&!c;if(void 0===t){if(a)activateLightMode();else if(o)activateDarkMode();else if(c||n){const e=(new Date).getHours();e<=6||e>=18?activateDarkMode():activateLightMode()}window.matchMedia("(prefers-color-scheme: dark)").addListener((function(e){void 0===saveToLocal.get("theme")&&(e.matches?activateDarkMode():activateLightMode())}))}else"light"===t?activateLightMode():activateDarkMode();const i=saveToLocal.get("aside-status");void 0!==i&&("hide"===i?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));const r=saveToLocal.get("global-font-size");void 0!==r&&document.documentElement.style.setProperty("--global-font-size",r+"px")})(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/css/custom/twikoo_beautify.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 5.4.1"><link rel="alternate" href="/atom.xml" title="梦家博客" type="application/atom+xml"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/avatar.gif" onerror='onerror=null,src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">183</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">63</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i> <span>娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i> <span>书单</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comments"></i> <span>留言</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://sites.google.com/site/mengjiashen01/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img13.jpeg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">梦家博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i> <span>娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i> <span>书单</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i> <span>电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comments"></i> <span>留言</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://sites.google.com/site/mengjiashen01/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch 简明学习教程与高效编程技巧</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-02-22T09:46:57.000Z" title="发表于 2019-02-22 09:46:57">2019-02-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-05-26T03:15:49.144Z" title="更新于 2022-05-26 03:15:49">2022-05-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/">技术杂谈</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/201902220946/#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="背景">背景</h2><p>邻近毕业季事情多且杂，已很长时间未接触PyTorch编程了，不动手实践以前学习的内容就慢慢淡忘了，而且现在PyTorch已更新到版本<code>1.5.0</code>，回想起最开始学习PyTorch时还是版本<code>0.4.1</code>只能感叹技术迭代和AI发展的速度太快了啊！以前学习的 tutorials 换电脑时也丢了…遂…</p><p>本篇文章首先简单过一遍PyTorch基础内容，再结合 Github 上的 tutorials 学习一些高效的PyTorch编程技巧！</p><p>对于初学者，当然首推PyTorch官方教程: <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/">PyTorch tutorials</a></p><h2 id="Pytorch-教程与实践">Pytorch 教程与实践</h2><p>基础内容可以直接阅读官方60分钟入门教程 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ</a></p><p>由于这个以前学习PyTorch时已经走过了，因此现在换条路走走了！</p><p>直接学习Github上的高效PyTorch编程技巧，此处学习大佬 <a target="_blank" rel="noopener" href="https://github.com/vahidk">Phd Vahid</a>，这位Phd Github中有两个仓库 <a target="_blank" rel="noopener" href="https://github.com/vahidk/EffectiveTensorflow">EffectiveTensorflow</a> 和 <a target="_blank" rel="noopener" href="https://github.com/vahidk/EffectivePyTorch">EffectivePyTorch</a>，其中EffectiveTensorflow库已接近8700 star，TensorFlow玩家可自取！本文学习下 <strong>EffectivePyTorch</strong> 教程。</p><p>本文的主要内容包括如下：</p><ul><li>PyTorch 基础</li><li>PyTorch 模型封装</li><li>PyTorch 广播机制的优缺点</li><li>PyTorch 重载运算符的使用</li><li>PyTorch TorchScript 运行时间优化</li><li>PyTorch 构造高效的 <code>dataloader</code> 类</li><li>PyTorch 数值稳定性</li></ul><p>包含代码说明的部分在 Jupyter 中体验更好哦，<a target="_blank" rel="noopener" href="https://nbviewer.jupyter.org/github/dreamhomes/awesome-programming-tutorials/blob/master/pytorch/effective-pytorch.ipynb">传送门~</a></p><h3 id="PyTorch-基础">PyTorch 基础</h3><p>PyTorch是最流行的用于数值计算的库之一，目前是用于进行机器学习研究的最广泛使用的库之一。在许多方面，PyTorch与NumPy相似，另外的好处是PyTorch允许您在CPU，GPU和TPU上执行计算，而无需对代码进行任何实质性更改。PyTorch还使您可以轻松地在多个设备或机器之间分布计算。PyTorch最重要的功能之一就是自动微分。它允许以有效的方式解析地计算函数的梯度，这对于使用梯度下降法训练机器学习模型至关重要。我们的目标是对PyTorch进行简要介绍，并讨论使用PyTorch的最佳实践。</p><p>了解PyTorch的第一件事是张量的概念。张量只是多维数组。PyTorch张量与NumPy数组非常相似，但其中增加了其它的函数功能。</p><p>一个张量 <code>tensor</code> 可以存储一个标量数值、一个数组、一个矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标量</span></span><br><span class="line">scalar = torch.tensor(<span class="number">3</span>)</span><br><span class="line">scalar</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数组</span></span><br><span class="line">array = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">array</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵</span></span><br><span class="line">matrix = torch.zeros([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">matrix</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 任意维度张量</span></span><br><span class="line">multi_tensor = torch.rand([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">multi_tensor</span><br></pre></td></tr></table></figure><p>张量可以高效的执行代数的运算。机器学习应用中最常见的运算就是矩阵乘法。例如将两个随机矩阵进行相乘，维度分别是 3x<br>5 和 5x4 ，这个运算可以通过矩阵相乘运算 <code>@</code> 实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn([<span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line">y = torch.randn([<span class="number">5</span>, <span class="number">4</span>])</span><br><span class="line">z = x @ y</span><br><span class="line">z</span><br></pre></td></tr></table></figure><p>可以通过 <code>x+y</code> 运算将两个 tensor 相加。</p><p>将 tensor 转为 numpy array 可以调用 <code>.numpy()</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.numpy()</span><br></pre></td></tr></table></figure><p>将 numpy array 转为 tensor 可以调用 <code>torch.tensor()</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = torch.tensor(np.random.normal([<span class="number">3</span>, <span class="number">5</span>]))</span><br><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>tensor([2.8194, 4.7365], dtype=torch.float64)
</code></pre><h2 id="PyTorch-自动微分">PyTorch 自动微分</h2><p>PyTorch 中相比 numpy 最大优点就是可以实现自动微分，对于优化神经网络参数的应用非常有帮助。本文通过举例来理解微分的过程。</p><p>假设复合函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(u(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>，可以通过链式法则计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span></span></span></span> 对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 的导数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>g</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>d</mi><mi>g</mi></mrow><mrow><mi>d</mi><mi>u</mi></mrow></mfrac><mo>∗</mo><mfrac><mrow><mi>d</mi><mi>u</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{d g}{d x}=\frac{d g}{d u} * \frac{d u}{d x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574399999999997em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:.03588em">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.0574399999999997em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:.03588em">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>以下说明 PyTorch 如何实现自动求导的过程。</p><p>为了在 PyTorch 中计算导数，首先要创建一个张量并设置其 <code>requires_grad = True</code>，然后利用张量运算来定义函数，假设<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">u</span></span></span></span> 是一个二次方的函数，而 <code>g</code> 是一个简单的线性函数:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">u</span>(<span class="params">x</span>):</span></span><br><span class="line">  <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span>(<span class="params">u</span>):</span></span><br><span class="line">  <span class="keyword">return</span> -u</span><br></pre></td></tr></table></figure><p>示例中复合函数是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">g(u(x))=-x^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.897438em;vertical-align:-.08333em"></span><span class="mord">−</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>，所以导数是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>2</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">-2x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">−</span><span class="mord">2</span><span class="mord mathnormal">x</span></span></span></span>，如果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> ，那么导数值为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">-2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">−</span><span class="mord">2</span></span></span></span> 。</p><p>在 PyTorch 中调用梯度函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dgdx = torch.autograd.grad(g(u(x)), x)[<span class="number">0</span>]</span><br><span class="line">dgdx</span><br></pre></td></tr></table></figure><pre><code>tensor(-2.)
</code></pre><h3 id="拟合曲线">拟合曲线</h3><p>为了展示自动微分的作用，此处介绍另一个例子。</p><p>首先假设有一些服从一个曲线（也就是函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>5</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">f(x)=5x^2+3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.897438em;vertical-align:-.08333em"></span><span class="mord">5</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span></span></span></span>）的样本，然后希望基于这些样本来评估这个函数 f(x) 。首先定义一个带参数的函数:</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">g(x, w)=w_{0} x^{2}+w_{1} x+w_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.0141079999999998em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8641079999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.73333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></span></p><p>函数的输入是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span>，然后<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span> 是参数，目标是找到合适的参数使得下列式子成立：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(x, w)=f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p>实现的一个方法可以是通过优化下面的损失函数来实现:</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mo>∑</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(w)=\sum(f(x)-g(x, w))^{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.6000100000000002em;vertical-align:-.55001em"></span><span class="mop op-symbol large-op" style="position:relative;top:-.000004999999999977245em">∑</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8641079999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p>此处定义的这个问题里有一个已知的函数即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> ，但可以采用一个更加通用的方法，可以应用到任何一个可微分的函数，并采用<strong>随机梯度下降法</strong>求解参数，即通过计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">)</span></span></span></span> 对于每个参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span> 的梯度的平均值。</p><p>在PyTorch实现过程如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming we know that the desired function is a polynomial of 2nd degree, we</span></span><br><span class="line"><span class="comment"># allocate a vector of size 3 to hold the coefficients and initialize it with</span></span><br><span class="line"><span class="comment"># random noise.</span></span><br><span class="line">w = torch.tensor(torch.randn([<span class="number">3</span>, <span class="number">1</span>]), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We use the Adam optimizer with learning rate set to 0.1 to minimize the loss.</span></span><br><span class="line">opt = torch.optim.Adam([w], <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment"># We define yhat to be our estimate of y.</span></span><br><span class="line">    f = torch.stack([x * x, x, torch.ones_like(x)], <span class="number">1</span>)</span><br><span class="line">    yhat = torch.squeeze(f @ w, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">y, yhat</span>):</span></span><br><span class="line">    <span class="comment"># The loss is defined to be the mean squared error distance between our</span></span><br><span class="line">    <span class="comment"># estimate of y and its true value.</span></span><br><span class="line">    loss = torch.nn.functional.mse_loss(yhat, y)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span>():</span></span><br><span class="line">    <span class="comment"># Generate some training data based on the true function</span></span><br><span class="line">    x = torch.rand(<span class="number">100</span>) * <span class="number">20</span> - <span class="number">10</span></span><br><span class="line">    y = <span class="number">5</span> * x * x + <span class="number">3</span></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>():</span></span><br><span class="line">    x, y = generate_data()</span><br><span class="line"></span><br><span class="line">    yhat = model(x)</span><br><span class="line">    loss = compute_loss(y, yhat)</span><br><span class="line"></span><br><span class="line">    opt.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    opt.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    train_step()</span><br><span class="line"></span><br><span class="line">w.detach().numpy()</span><br></pre></td></tr></table></figure><pre><code>array([[ 4.9990711e+00],
       [-1.3374003e-04],
       [ 3.0566640e+00]], dtype=float32)
</code></pre><p>从结果可知与定义函数的参数非常接近，以上仅是简单的函数PyTorch可以拟合更复杂的函数。很多问题，比如优化一个带有上百万参数的神经网络，都可以用 PyTorch 高效实现，PyTorch 可以跨多个设备和线程进行拓展，并且支持多个平台。</p><h2 id="PyTorch-模型封装">PyTorch 模型封装</h2><p>在前面的示例中，我们直接使用了张量和张量操作来构建模型。为了使代码更有条理，建议使用PyTorch的模块。模块只是参数的容器，并且封装了模型操作。例如表示线性模型<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>a</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = ax + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">b</span></span></span></span>。该模型可以用以下代码表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.a = torch.nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line">    self.b = torch.nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    yhat = self.a * x + self.b</span><br><span class="line">    <span class="keyword">return</span> yhat</span><br></pre></td></tr></table></figure><p>使用的例子如下所示，需要实例化声明的模型，并且像调用函数一样使用它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">10</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">y = net(x)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><p>参数都是设置 <code>requires_grad</code> 为 <code>true</code> 的张量。通过模型的 <code>parameters()</code> 方法可以很方便的访问和使用参数，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br></pre></td></tr></table></figure><p>假设是一个未知的函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>5</mn><mi>x</mi><mo>+</mo><mn>3</mn><mo>+</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">y=5x+3+n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">5</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> ，注意这里的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> 是表示噪音，然后希望优化模型参数来拟合这个函数，首先可以简单从这个函数进行采样，得到一些样本数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">100</span>, dtype=torch.float32) / <span class="number">100</span></span><br><span class="line">y = <span class="number">5</span> * x + <span class="number">3</span> + torch.rand(<span class="number">100</span>) * <span class="number">0.3</span></span><br><span class="line">x, y</span><br></pre></td></tr></table></figure><p>和上一个例子类似，需要定义一个损失函数并优化模型的参数，如下所示:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">  net.zero_grad()</span><br><span class="line">  yhat = net(x)</span><br><span class="line">  loss = criterion(yhat, y)</span><br><span class="line">  loss.backward()</span><br><span class="line">  optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net.a, net.b) <span class="comment"># Should be close to 5 and 3</span></span><br></pre></td></tr></table></figure><pre><code>Parameter containing:
tensor([4.9549], requires_grad=True) Parameter containing:
tensor([3.1692], requires_grad=True)
</code></pre><p>在 PyTorch 中已经实现了很多预定义好的模块。比如 <code>torch.nn.Linear</code> 就是一个类似上述例子中定义的一个更加通用的线性函数，所以我们可以采用这个函数来重写模型代码，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    yhat = self.linear(x.unsqueeze(<span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> yhat</span><br></pre></td></tr></table></figure><p>此处使用两个函数，<code>squeeze</code> 和 <code>unsqueeze</code> ，主要是 <code>torch.nn.Linear</code> 会对一批向量而不是数值进行操作。</p><p>默认调用 <code>parameters()</code> 会返回其所有子模块的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = Net()</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br></pre></td></tr></table></figure><pre><code>Parameter containing:
tensor([[0.9434]], requires_grad=True)
Parameter containing:
tensor([0.9605], requires_grad=True)
</code></pre><p>可以预定义模块作为包容其他模块的容器，最常用的就是 <code>torch.nn.Sequential</code> ，它的名字就暗示了它主要用于堆叠多个模块（或者网络层），例如堆叠两个线性网络层，中间是一个非线性函数 <code>ReLU</code> ，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">64</span>, <span class="number">32</span>),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(<span class="number">32</span>, <span class="number">10</span>),</span><br><span class="line">)</span><br><span class="line">model</span><br></pre></td></tr></table></figure><pre><code>Sequential(
  (0): Linear(in_features=64, out_features=32, bias=True)
  (1): ReLU()
  (2): Linear(in_features=32, out_features=10, bias=True)
)
</code></pre><h2 id="PyTorch-广播机制的优缺点">PyTorch 广播机制的优缺点</h2><h3 id="优点">优点</h3><p>PyTorch 支持广播的元素积运算。正常情况下，当想执行类似加法和乘法操作的时候，你需要确认操作数的形状是匹配的，比如无法进行一个 [3, 2] 大小的张量和 [3, 4] 大小的张量的加法操作。</p><p>但是存在一种特殊的情况：只有单一维度的时候，PyTorch 会隐式的根据另一个操作数的维度来拓展只有单一维度的操作数张量。因此，实现 [3,2] 大小的张量和 [3,1] 大小的张量相加的操作是合法的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">1.</span>], [<span class="number">2.</span>]])</span><br><span class="line"><span class="comment"># c = a + b.repeat([1, 2])</span></span><br><span class="line"><span class="comment"># 不同维度可以直接相加</span></span><br><span class="line">c = a + b</span><br><span class="line">c</span><br></pre></td></tr></table></figure><pre><code>tensor([[2., 3.],
        [5., 6.]])
</code></pre><p>广播机制可以实现<strong>隐式</strong>的维度复制操作（<code>repeat</code> 操作），并且代码更短，内存使用上也更加高效，因为不需要存储复制的数据的结果。这个机制非常适合用于结合多个维度不同的特征的时候。</p><p>为了拼接不同维度的特征，通常的做法是先对输入张量进行维度上的复制，然后拼接后使用非线性激活函数。整个过程的代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand([<span class="number">5</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line">b = torch.rand([<span class="number">5</span>, <span class="number">1</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">linear = torch.nn.Linear(<span class="number">11</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># concat a and b and apply nonlinearity</span></span><br><span class="line">tiled_b = b.repeat([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>]) <span class="comment"># b shape:  [5, 3, 6]</span></span><br><span class="line">c = torch.cat([a, tiled_b], <span class="number">2</span>) <span class="comment"># c shape: [5, 3, 11]</span></span><br><span class="line">d = torch.nn.functional.relu(linear(c))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(d.shape)  <span class="comment"># torch.Size([5, 3, 10])</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([5, 3, 10])
</code></pre><p>但实际上通过广播机制可以实现得更加高效，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(m(x+y))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> 是等同于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>m</mi><mi>x</mi><mo>+</mo><mi>m</mi><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(mx+my)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mclose">)</span></span></span></span> 的，也就是我们可以先分别做线性操作，然后通过广播机制来做隐式的拼接操作，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand([<span class="number">5</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line">b = torch.rand([<span class="number">5</span>, <span class="number">1</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">linear1 = torch.nn.Linear(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">pa = linear1(a) <span class="comment"># pa shape: [5, 3, 10]</span></span><br><span class="line">pb = linear2(b) <span class="comment"># pb shape: [5, 1, 10]</span></span><br><span class="line">d = torch.nn.functional.relu(pa + pb)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(d.shape)  <span class="comment"># torch.Size([5, 3, 10])</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([5, 3, 10])
</code></pre><p>实际上这段代码非常通用，可以用于任意维度大小的张量，只要它们之间是可以实现广播机制的，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Merge</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features1, in_features2, out_features, activation=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(in_features1, out_features)</span><br><span class="line">        self.linear2 = torch.nn.Linear(in_features2, out_features)</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, a, b</span>):</span></span><br><span class="line">        pa = self.linear1(a)</span><br><span class="line">        pb = self.linear2(b)</span><br><span class="line">        c = pa + pb</span><br><span class="line">        <span class="keyword">if</span> self.activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            c = self.activation(c)</span><br><span class="line">        <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h3 id="缺点">缺点</h3><p>上述内容是广播机制的优点。但它的缺点是什么呢？原因也是出现在隐式的操作，这种做法非常不利于进行代码的调试。<br>以例子说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1.</span>], [<span class="number">2.</span>]])</span><br><span class="line">b = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line">c = torch.<span class="built_in">sum</span>(a + b)</span><br><span class="line">c</span><br></pre></td></tr></table></figure><pre><code>tensor(12.)
</code></pre><p>所以上述代码的输出结果 c 是什么呢？你可能觉得是 6，但这是错的，正确答案是 12 。这是因为当两个张量的维度不匹配的时候，PyTorch 会自动将维度低的张量的第一个维度进行拓展，然后在进行元素之间的运算，所以这里会将b 先拓展为 [[1, 2], [1, 2]]，然后 a+b 的结果应该是 [[2,3], [3, 4]] ，然后sum 操作是将所有元素求和得到结果 12。</p><p>那么避免这种结果的方法就是<strong>显式</strong>的操作，比如在这个例子中就需要指定好想要求和的维度，这样进行代码调试会更简单，代码修改后如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1.</span>], [<span class="number">2.</span>]])</span><br><span class="line">b = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line">c = torch.<span class="built_in">sum</span>(a + b, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">c</span><br></pre></td></tr></table></figure><pre><code>tensor([5., 7.])
</code></pre><p>这里得到的 c 的结果是 [5, 7]，而我们基于结果的维度可以知道出现了错误。</p><p>这有个通用的做法，就是在做累加（ reduction ）操作或者使用 <code>torch.squeeze</code> 的时候总是指定好维度。</p><h2 id="PyTorch-重载运算符的使用">PyTorch 重载运算符的使用</h2><p>与 NumPy 类似，PyTorch 会重载 python 的一些<strong>运算符</strong>来让 PyTorch 代码更简短和更有可读性。</p><p>例如，切片操作就是其中一个重载的运算符，可以更容易的对张量进行索引操作，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">5</span>)</span><br><span class="line">begin = <span class="number">1</span></span><br><span class="line">end = <span class="number">3</span></span><br><span class="line">z = x[begin:end]</span><br><span class="line">x, z</span><br></pre></td></tr></table></figure><pre><code>(tensor([ 1.5239,  1.0537,  0.6141, -0.1963, -0.9043]),
 tensor([1.0537, 0.6141]))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 等同于narrow</span></span><br><span class="line">z = torch.narrow(x, <span class="number">0</span>, begin, end-begin)</span><br><span class="line">z</span><br></pre></td></tr></table></figure><pre><code>tensor([1.0537, 0.6141])
</code></pre><p>但需要谨慎使用这个运算符，过度使用可以导致代码变得低效。以下示例说明该运算符如何使代码低效！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵行累加</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">x = torch.rand([<span class="number">500</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">z = torch.zeros([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    z += x[i]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Took %f seconds.&quot;</span> % (time.time() - start))</span><br></pre></td></tr></table></figure><pre><code>Took 0.004023 seconds.
</code></pre><p>上述代码的运行速度会变慢，因为总共调用了 500 次的切片操作，这就是过度使用。一个更好的做法是采用 <code>torch.unbind</code> 运算符在每次循环中将矩阵切片为一个向量的列表，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand([<span class="number">500</span>, <span class="number">10</span>])</span><br><span class="line">z = torch.zeros([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> x_i <span class="keyword">in</span> torch.unbind(x):</span><br><span class="line">    z += x_i</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Took %f seconds.&quot;</span> % (time.time() - start))</span><br></pre></td></tr></table></figure><pre><code>Took 0.003989 seconds.
</code></pre><p>原文作者认为<code>torch.unbind</code>改进会提高一些速度。但从上文结果看出优化效果并不大，正确的做法应该是采用 <code>torch.sum</code> 来一步实现累加的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start = time.time()</span><br><span class="line">z = torch.<span class="built_in">sum</span>(x, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Took %f seconds.&quot;</span> % (time.time() - start))</span><br></pre></td></tr></table></figure><pre><code>Took 0.000998 seconds.
</code></pre><p>调用<code>torch.sum()</code>实现累加速度就非常快了啊！</p><p>其他重载的算数和逻辑运算符分别是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">z = -x  <span class="comment"># z = torch.neg(x)</span></span><br><span class="line">z = x + y  <span class="comment"># z = torch.add(x, y)</span></span><br><span class="line">z = x - y</span><br><span class="line">z = x * y  <span class="comment"># z = torch.mul(x, y)</span></span><br><span class="line">z = x / y  <span class="comment"># z = torch.div(x, y)</span></span><br><span class="line">z = x // y</span><br><span class="line">z = x % y</span><br><span class="line">z = x ** y  <span class="comment"># z = torch.pow(x, y)</span></span><br><span class="line">z = x @ y  <span class="comment"># z = torch.matmul(x, y)</span></span><br><span class="line">z = x &gt; y</span><br><span class="line">z = x &gt;= y</span><br><span class="line">z = x &lt; y</span><br><span class="line">z = x &lt;= y</span><br><span class="line">z = <span class="built_in">abs</span>(x)  <span class="comment"># z = torch.abs(x)</span></span><br><span class="line">z = x &amp; y</span><br><span class="line">z = x | y</span><br><span class="line">z = x ^ y  <span class="comment"># z = torch.logical_xor(x, y)</span></span><br><span class="line">z = ~x  <span class="comment"># z = torch.logical_not(x)</span></span><br><span class="line">z = x == y  <span class="comment"># z = torch.eq(x, y)</span></span><br><span class="line">z = x != y  <span class="comment"># z = torch.ne(x, y)</span></span><br></pre></td></tr></table></figure><p>可以使用这些运算符的递增版本，比如 <code>x += y</code> 和 <code>x **=2</code> 都是合法的。另外，Python 并不允许重载 <code>and</code> 、<code>or</code> 和 <code>not</code> 三个关键词。</p><h2 id="PyTorch-TorchScript-运行时间优化">PyTorch TorchScript 运行时间优化</h2><p>PyTorch 优化了高维度的张量的运算。在 PyTorch 中对小张量进行太多的运算操作是非常低效的。所以有可能的话，将计算操作都重写为批次（batch）的形式，可以减少消耗和提高性能。而如果没办法自己手动实现批次的运算操作，那么可以采用 TorchScript 来提升代码的性能。</p><p>TorchScript 是一个 Python 函数的子集，但经过了 PyTorch 的验证，PyTorch 可以通过其 <strong>just in time(jit)</strong> 编译器来自动优化 TorchScript 代码，提高性能。</p><p>以具体例子说明。在机器学习应用中非常常见的操作就是 <code>batch gather</code> ，也就是 <code>output[i] = input[i, index[i]]</code>可以理解为收集特定位置的值。其代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gather</span>(<span class="params">tensor, indices</span>):</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tensor.size(<span class="number">0</span>)):</span><br><span class="line">        output += [tensor[i][indices[i]]]</span><br><span class="line">    <span class="keyword">return</span> torch.stack(output)</span><br></pre></td></tr></table></figure><p>通过 <code>torch.jit.script</code> 装饰器来使用 TorchScript 的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.jit.script</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gather_jit</span>(<span class="params">tensor, indices</span>):</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tensor.size(<span class="number">0</span>)):</span><br><span class="line">        output += [tensor[i][indices[i]]]</span><br><span class="line">    <span class="keyword">return</span> torch.stack(output)</span><br></pre></td></tr></table></figure><p>这个做法可以提高 10% 的运算速度(未验证)。 但更好的做法还是手动实现批次的运算操作，下面是一个向量化实现的代码例子，提高了 100 倍的速度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gather_vec</span>(<span class="params">tensor, indices</span>):</span></span><br><span class="line">    shape = <span class="built_in">list</span>(tensor.shape)</span><br><span class="line">    flat_first = torch.reshape(</span><br><span class="line">        tensor, [shape[<span class="number">0</span>] * shape[<span class="number">1</span>]] + shape[<span class="number">2</span>:])</span><br><span class="line">    offset = torch.reshape(</span><br><span class="line">        torch.arange(shape[<span class="number">0</span>]).cuda() * shape[<span class="number">1</span>],</span><br><span class="line">        [shape[<span class="number">0</span>]] + [<span class="number">1</span>] * (<span class="built_in">len</span>(indices.shape) - <span class="number">1</span>))</span><br><span class="line">    output = flat_first[indices + offset]</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><strong>PS：这部分其实可以直接调用 <code>torch.gather()</code> 函数实现效果</strong>，参考另一篇文章<a target="_blank" rel="noopener" href="https://blog.csdn.net/DreamHome_S/article/details/106032920">PyTorch 函数解释：torch.gather()</a></p><h2 id="PyTorch-构建高效-dataloader-类">PyTorch 构建高效 dataloader 类</h2><p>如需代码运行更快，可以考虑如何将数据更加高效加载到内存中。PyTorch 提供了一个很容易加载数据的工具，即 <code>DataLoader</code> 。<code>DataLoader</code> 会采用多个 <code>workers</code> 来同时将数据从 <code>Dataset</code> 类中加载，并且可以选择使用 <code>Sampler</code> 类来对采样数据和组成 <code>batch</code> 形式的数据。</p><p>如果你可以随时访问你的数据，那么使用 DataLoader 会非常简单：只需要继承 <code>Dataset</code> 类别并实现 <code>__getitem__</code> (读取每个数据)和 <code>__len__</code>（返回数据集的样本数量）这两个方法。</p><p>下面给出一个代码例子，如何从给定的文件夹中加载图片数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageDirectoryDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">path, pattern</span>):</span></span><br><span class="line">        self.paths = <span class="built_in">list</span>(glob.glob(os.path.join(path, pattern)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.paths)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__item__</span>(<span class="params">self</span>):</span></span><br><span class="line">        path = random.choice(paths)</span><br><span class="line">        <span class="keyword">return</span> cv2.imread(path, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>比如想将文件夹内所有的 jpeg 图片都加载，代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataloader = torch.utils.data.DataLoader(ImageDirectoryDataset(<span class="string">&quot;/data/imagenet/*.jpg&quot;</span>), num_workers=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="comment"># do something with data</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>这里采用了 8 个 <code>workers</code> 来并行的从硬盘中读取数据。这个数量可以根据实际使用机器来进行调试，得到一个最佳的数量。</p><p>当你的数据都很大或者你的硬盘读写速度很快，采用DataLoader进行随机读取数据是可行的。但也可能存在一种情况，就是使用的是一个很慢的连接速度的网络文件系统，请求单个文件的速度都非常的慢，而这可能就是整个训练过程中的瓶颈。</p><p>一个更好的做法就是将数据保存为一个可以连续读取的连续文件格式。例如，当你有非常大量的图片数据，可以采用 tar 命令将其压缩为一个文件，然后用 python 来从这个压缩文件中连续的读取图片。要实现这个操作，需要用到 PyTorch 的 <code>IterableDataset</code>。创建一个 IterableDataset 类，只需要实现 <code>__iter__</code> 方法即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tar_image_iterator</span>(<span class="params">path</span>):</span></span><br><span class="line">    tar = tarfile.<span class="built_in">open</span>(self.path, <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> tar_info <span class="keyword">in</span> tar:</span><br><span class="line">        file = tar.extractfile(tar_info)</span><br><span class="line">        content = file.read()</span><br><span class="line">        <span class="keyword">yield</span> cv2.imdecode(content, <span class="number">1</span>)</span><br><span class="line">        file.close()</span><br><span class="line">        tar.members = []</span><br><span class="line">    tar.close()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TarImageDataset</span>(<span class="params">torch.utils.data.IterableDataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, path</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.path = path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> tar_image_iterator(self.path)</span><br></pre></td></tr></table></figure><p>此方法有一个问题，当使用 DataLoader 以及多个 workers 读取这个数据集的时候，会得到很多重复的数据。这个问题主要是因为每个 <code>worker</code> 都会创建一个单独的数据集的实例，并且都是从数据集的起始位置开始读取数据。一种避免这个问题的办法就是不是压缩为一个 tar 文件，而是将数据划分成 num_workers 个单独的 tar 文件，然后每个 worker 分别加载一个，代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TarImageDataset</span>(<span class="params">torch.utils.data.IterableDataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, paths</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.paths = paths</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        worker_info = torch.utils.data.get_worker_info()</span><br><span class="line">        <span class="comment"># For simplicity we assume num_workers is equal to number of tar files</span></span><br><span class="line">        <span class="keyword">if</span> worker_info <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> worker_info.num_workers != <span class="built_in">len</span>(self.paths):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Number of workers doesn&#x27;t match number of files.&quot;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> tar_image_iterator(self.paths[worker_info.worker_id])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    TarImageDataset([<span class="string">&quot;/data/imagenet_part1.tar&quot;</span>, <span class="string">&quot;/data/imagenet_part2.tar&quot;</span>]), num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="comment"># do something with data</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="PyTorch-数值稳定性">PyTorch 数值稳定性</h2><p>在我们使用任意一个数值计算库时，比如 NumPy 或者 PyTorch ，都需要知道一点，编写数学上正确的代码不一定会得到正确的结果，你需要确保这个计算是稳定的。</p><p>以实例说明。从数学上来说，对任意的非零<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> ，可知式子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∗</mo><mi>y</mi><mi mathvariant="normal">/</mi><mi>y</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">x*y/y=x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.46528em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 是成立的。 再看具体实现时候的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.float32(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y = np.float32(<span class="number">1e-50</span>)  <span class="comment"># y would be stored as zero</span></span><br><span class="line">z = x * y / y</span><br><span class="line"></span><br><span class="line">z</span><br></pre></td></tr></table></figure><pre><code>nan
</code></pre><p>代码的运行结果是打印 <code>nan</code> ，原因是 y 的数值对于 float32 类型来说非常的小，这导致它的实际数值是 0 而不是 1e-50。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = np.float32(<span class="number">1e39</span>)  <span class="comment"># y would be stored as inf</span></span><br><span class="line">z = x * y / y</span><br><span class="line"></span><br><span class="line">z</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">D:\DevTools\Miniconda3\lib\site-packages\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in float_scalars</span><br><span class="line"></span><br><span class="line">nan</span><br></pre></td></tr></table></figure><p>对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span>非常大的情况输出结果依然是 nan ，因为 y 太大而被存储为 inf 的情况，对于 float32 类型来说，其范围是 <code>1.4013e-45 ~ 3.40282e+38</code>，当超过这个范围，就会被置为 0 或者 inf。</p><p>如何查看一种数据类型的数值范围：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.nextafter(np.float32(<span class="number">0</span>), np.float32(<span class="number">1</span>)))  <span class="comment"># prints 1.4013e-45</span></span><br><span class="line"><span class="built_in">print</span>(np.finfo(np.float32).<span class="built_in">max</span>)  <span class="comment"># print 3.40282e+38</span></span><br><span class="line"><span class="built_in">print</span>(np.finfo(np.float32).<span class="built_in">min</span>)</span><br></pre></td></tr></table></figure><pre><code>1e-45
3.4028235e+38
-3.4028235e+38
</code></pre><p>为了让计算变得稳定，需要避免过大或者过小的数值。这看起来很容易，但这类问题是很难进行调试，特别是在 PyTorch 中进行梯度下降的时候。这不仅因为需要确保在前向传播过程中的所有数值都在使用的数据类型的取值范围内，还要保证在反向传播中也做到这一点。</p><p>下面给出一个代码例子，计算一个输出向量的 softmax，一种不好的代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unstable_softmax</span>(<span class="params">logits</span>):</span></span><br><span class="line">    exp = torch.exp(logits)</span><br><span class="line">    <span class="keyword">return</span> exp / torch.<span class="built_in">sum</span>(exp)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(unstable_softmax(torch.tensor([<span class="number">1000.</span>, <span class="number">0.</span>])).numpy())  <span class="comment"># prints [ nan, 0.]</span></span><br></pre></td></tr></table></figure><pre><code>[nan  0.]
</code></pre><p>计算 logits 的指数数值可能会得到超出 float32 类型的取值范围，即过大或过小的数值，这里最大的 logits 数值是 <code>ln(3.40282e+38) = 88.7</code>，超过这个数值都会导致 nan 。</p><p>那么应该如何避免这种情况，做法很简单。因为有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∑</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∑</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\exp (x-c)}{\sum \exp (x-c)}=\frac{\exp (x)}{\sum \exp (x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.530007em;vertical-align:-.520007em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-.0000050000000000050004em">∑</span><span class="mspace mtight" style="margin-right:.19516666666666668em"></span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">c</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">c</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.520007em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.530007em;vertical-align:-.520007em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-.0000050000000000050004em">∑</span><span class="mspace mtight" style="margin-right:.19516666666666668em"></span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.520007em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，也就是我们可以对 logits 减去一个常量，但结果保持不变，所以我们选择 logits 的最大值作为这个常数，这种做法，指数函数的取值范围就会限制为 [-inf, 0] ，然后最终的结果就是 [0.0, 1.0] 的范围，代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">logits</span>):</span></span><br><span class="line">    exp = torch.exp(logits - torch.<span class="built_in">max</span>(logits))</span><br><span class="line">    <span class="keyword">return</span> exp / torch.<span class="built_in">sum</span>(exp)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(softmax(torch.tensor([<span class="number">1000.</span>, <span class="number">0.</span>])).numpy())  <span class="comment"># prints [ 1., 0.]</span></span><br></pre></td></tr></table></figure><pre><code>[1. 0.]
</code></pre><p>假设现在有一个分类问题。采用 softmax 函数对输出值 logits 计算概率。接着定义采用预测值和标签的交叉熵作为损失函数。对于一个类别分布的交叉熵可以简单定义为 ：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>x</mi><mi>e</mi><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo>∑</mo><msub><mi>p</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">x e(p, q)=-\sum p_{i} \log \left(q_{i}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">x</span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.03588em">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.6000100000000002em;vertical-align:-.55001em"></span><span class="mord">−</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-symbol large-op" style="position:relative;top:-.000004999999999977245em">∑</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0">)</span></span></span></span></span></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不稳定地实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unstable_softmax_cross_entropy</span>(<span class="params">labels, logits</span>):</span></span><br><span class="line">    logits = torch.log(softmax(logits))</span><br><span class="line">    <span class="keyword">return</span> -torch.<span class="built_in">sum</span>(labels * logits)</span><br><span class="line"></span><br><span class="line">labels = torch.tensor([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">logits = torch.tensor([<span class="number">1000.</span>, <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line">xe = unstable_softmax_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(xe.numpy())  <span class="comment"># prints inf</span></span><br></pre></td></tr></table></figure><pre><code>inf
</code></pre><p>在上述代码实现中，当 softmax 结果趋向于 0，其 log 输出会趋向于无穷，这就导致计算结果的不稳定性。所以可以对其进行重写，将 softmax 维度拓展并做一些归一化的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_cross_entropy</span>(<span class="params">labels, logits, dim=-<span class="number">1</span></span>):</span></span><br><span class="line">    scaled_logits = logits - torch.<span class="built_in">max</span>(logits)</span><br><span class="line">    normalized_logits = scaled_logits - torch.logsumexp(scaled_logits, dim)</span><br><span class="line">    <span class="keyword">return</span> -torch.<span class="built_in">sum</span>(labels * normalized_logits)</span><br><span class="line"></span><br><span class="line">labels = torch.tensor([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">logits = torch.tensor([<span class="number">1000.</span>, <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line">xe = softmax_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(xe.numpy())  <span class="comment"># prints 500.0</span></span><br></pre></td></tr></table></figure><pre><code>500.0
</code></pre><p>可以验证计算的梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">logits.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">xe = softmax_cross_entropy(labels, logits)</span><br><span class="line">g = torch.autograd.grad(xe, logits)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(g.numpy())  <span class="comment"># prints [0.5, -0.5]</span></span><br></pre></td></tr></table></figure><pre><code>[ 0.5 -0.5]
</code></pre><p>再次提醒，进行梯度下降操作的时候需要额外的小心谨慎，需要确保每个网络层的函数和梯度的范围都在合法的范围内，指数函数和对数函数在不正确使用的时候都可能导致很大的问题，它们都能将非常小的数值转换为非常大的数值，或者从很大变为很小的数值。</p><h2 id="结论">结论</h2><p>目前暂时更新这么多内容，日后更新！</p><h2 id="联系作者">联系作者</h2><img src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200504171717935.gif" style="zoom:50%"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">梦家</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://dreamhomes.top/posts/201902220946/">https://dreamhomes.top/posts/201902220946/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dreamhomes.top" target="_blank">梦家博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img13.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/wechat.png" target="_blank"><img class="post-qr-code-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/wechat.png" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/alipay.jpg" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/201902220959/"><img class="prev-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg" onerror='onerror=null,src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">使用 jsDelivr CDN 对 Github 图床进行加速</div></div></a></div><div class="next-post pull-right"><a href="/posts/201902202120/"><img class="next-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img12.jpeg" onerror='onerror=null,src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【2019/CVPR】3D Hand Shape and Pose Estimation from a Single RGB Image</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/201812302155/" title="PyTorch 高效编程手册"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img7.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-30</div><div class="title">PyTorch 高效编程手册</div></div></a></div><div><a href="/posts/201906081516/" title="PyTorch 常用函数解析"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-08</div><div class="title">PyTorch 常用函数解析</div></div></a></div><div><a href="/posts/202011261041/" title="PyTorch 中的损失函数总结"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img5.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-26</div><div class="title">PyTorch 中的损失函数总结</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/avatar.gif" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/friend_404.gif"' alt="avatar"><div class="author-info__name">梦家</div><div class="author-info__description">算法工程师｜机器学习<br><br>前 途 似 海 渺 渺<br>来 日 方 长 漫 漫<br>与 其 互 为 人 间<br>不 如 自 成 宇 宙</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">183</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">63</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dreamhomes"><i class="fab fa-github"></i><span>Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/dreamhomes" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:shenmj13@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/DreamHome_S" target="_blank" title="CSDN"><i class="fab fa-cuttlefish"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>重要通知</span></div><div class="announcement_content">如对本站内容存在任何疑问 🧐 欢迎 <a href="https://dreamhomes.top/comments/">留言</a> 反馈或者联系作者<br><br><center><img src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/20210910100633.png" alt="联系博主" style="zoom:72%"></center></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-%E6%95%99%E7%A8%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5"><span class="toc-number">2.</span> <span class="toc-text">Pytorch 教程与实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-%E5%9F%BA%E7%A1%80"><span class="toc-number">2.1.</span> <span class="toc-text">PyTorch 基础</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">3.</span> <span class="toc-text">PyTorch 自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%9F%E5%90%88%E6%9B%B2%E7%BA%BF"><span class="toc-number">3.1.</span> <span class="toc-text">拟合曲线</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E6%A8%A1%E5%9E%8B%E5%B0%81%E8%A3%85"><span class="toc-number">4.</span> <span class="toc-text">PyTorch 模型封装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">5.</span> <span class="toc-text">PyTorch 广播机制的优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">5.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">5.2.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E9%87%8D%E8%BD%BD%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">6.</span> <span class="toc-text">PyTorch 重载运算符的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-TorchScript-%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4%E4%BC%98%E5%8C%96"><span class="toc-number">7.</span> <span class="toc-text">PyTorch TorchScript 运行时间优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E6%9E%84%E5%BB%BA%E9%AB%98%E6%95%88-dataloader-%E7%B1%BB"><span class="toc-number">8.</span> <span class="toc-text">PyTorch 构建高效 dataloader 类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-number">9.</span> <span class="toc-text">PyTorch 数值稳定性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">10.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%94%E7%B3%BB%E4%BD%9C%E8%80%85"><span class="toc-number">11.</span> <span class="toc-text">联系作者</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/202204281516/" title="智能运维 AIOps 系列丨调用链根因定位算法综述"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img12.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="智能运维 AIOps 系列丨调用链根因定位算法综述"></a><div class="content"><a class="title" href="/posts/202204281516/" title="智能运维 AIOps 系列丨调用链根因定位算法综述">智能运维 AIOps 系列丨调用链根因定位算法综述</a><time datetime="2022-04-28T15:16:58.000Z" title="发表于 2022-04-28 15:16:58">2022-04-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204241101/" title="Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img6.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析"></a><div class="content"><a class="title" href="/posts/202204241101/" title="Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析">Chart Studio &amp; Datapane：优雅地将 Plotly 图嵌入到 Markdown 文件中进行可视化分析</a><time datetime="2022-04-24T11:01:59.000Z" title="发表于 2022-04-24 11:01:59">2022-04-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204221627/" title="ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img9.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法"></a><div class="content"><a class="title" href="/posts/202204221627/" title="ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法">ICSE 2022丨DeepTraLog：GNN 和 Trace-Log 数据相结合的微服务异常检测方法</a><time datetime="2022-04-22T16:27:43.000Z" title="发表于 2022-04-22 16:27:43">2022-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204221003/" title="齐夫定律 (Zipf' law) 理论及其应用场景"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img6.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="齐夫定律 (Zipf' law) 理论及其应用场景"></a><div class="content"><a class="title" href="/posts/202204221003/" title="齐夫定律 (Zipf' law) 理论及其应用场景">齐夫定律 (Zipf' law) 理论及其应用场景</a><time datetime="2022-04-22T10:03:07.000Z" title="发表于 2022-04-22 10:03:07">2022-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/202204201700/" title="大规模图可视化分析框架 Graphistry"><img data-lazy-src="https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img2.jpeg" onerror='this.onerror=null,this.src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/404.jpg"' alt="大规模图可视化分析框架 Graphistry"></a><div class="content"><a class="title" href="/posts/202204201700/" title="大规模图可视化分析框架 Graphistry">大规模图可视化分析框架 Graphistry</a><time datetime="2022-04-20T17:00:42.000Z" title="发表于 2022-04-20 17:00:42">2022-04-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://cdn.jsdelivr.net/gh/dreamhomes/blog-image-bed@master/top/dreamhomes/butterflyblog/imgs/img13.jpeg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 梦家</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/"><img src="https://cdn.jsdelivr.net/gh/dreamhomes/dreamhomes.github.io@master/img/icp.png"> <span>赣ICP备 20003744 号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>document.querySelectorAll("#article-container span.katex-display").forEach(a=>{btf.wrap(a,"div","","katex-wrap")})</script><script>document.getElementsByClassName("mermaid").length&&(window.mermaidJsLoad?mermaid.init():getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(()=>{window.mermaidJsLoad=!0,mermaid.initialize({theme:"default"})}))</script><script>(()=>{const o=document.getElementById("twikoo-count"),t=()=>{let o={el:"#twikoo-wrap",envId:"hello-cloudbase-7gmvk17h649821e3",region:""};twikoo.init(o)},e=()=>{twikoo.getCommentsCount({envId:"hello-cloudbase-7gmvk17h649821e3",region:"",urls:[window.location.pathname],includeReply:!1}).then((function(t){o.innerText=t[0].count})).catch((function(o){console.error(o)}))},n=(n=!1)=>{"object"==typeof twikoo?(t(),n&&o&&setTimeout(e,0)):getScript("https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js").then(()=>{t(),n&&o&&setTimeout(e,0)})};n(!0)})()</script><script>function loadValine(){function e(){let e={el:"#vcomment",appId:"1JeI4r7MuSNuHIKgevu6uP8q-gzGzoHsz",appKey:"Ykl58TSExNWvTIyBTbnmp26h",placeholder:"💗 Hi, see you tomorrow. 💗",avatar:"monsterid",meta:"nick,mail,link".split(","),pageSize:"10",lang:"zh-CN",recordIP:!1,serverURLs:"https://1jei4r7m.lc-cn-n1-shared.com",emojiCDN:"",emojiMaps:"",enableQQ:!1,path:window.location.pathname,master:"7a7f78ed775ede7059cf68eb46898cc5".split(","),friends:"5d7570d1f1fefc9918c797ee65af3899,ec18c657f7ea8128e4bb73e96e45d236,d304727a8d439e45b08f8579666ee818".split(","),tagMeta:"博主,伙伴,访客".split(",")};e.requiredFields="nick,mail".split(",");new Valine(e)}"function"==typeof Valine?e():getScript("https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js").then(e)}{function loadOtherComment(){loadValine()}setTimeout(loadValine,0)}</script></div><script defer id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>function gitcalendar_injector_config(){document.getElementById("recent-posts").insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>'),console.log("已挂载gitcalendar")}document.getElementById("recent-posts")&&"/"===location.pathname&&(gitcalendar_injector_config(),GitCalendarInit("https://gitcalendar.akilar.top/api?dreamhomes",["#ebedf0","#fdcdec","#fc9bd9","#fa6ac5","#f838b2","#f5089f","#c4067e","#92055e","#540336","#48022f","#30021f"],"dreamhomes"))</script></body></html>